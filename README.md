
# ðŸš¨ Risk Behavior Detection System for Drivers

This project uses **YOLOv5, YOLOv8, YOLOv11, and YOLOv12** (compatible formats: `.pt` [PyTorch], `.tflite` [TensorFlow Lite float32, float16, int8, and EdgeTPU]), along with **MediaPipe**, **Dlib**, and **OpenCV** to detect dangerous driver behaviors such as:

- Drowsiness  
- Mobile phone usage  
- No seatbelt  
- Excessive body leaning  
- Smoking  

âš ï¸ When suspicious behavior is detected, an **audible alert** is automatically triggered.

---

## âœ… Features

- Object detection using **YOLO** (`drowsy`, `phone`, `smoking`, `seatbelt`, `awake`)
- **Eye closure detection** using Dlib (facial landmarks and Blink Ratio â€” BR)
- **Posture analysis** using MediaPipe (calculating Nose-to-Shoulder Center ratio â€” DSC)
- **Automatic audible alert** using `playsound`
- Real-time interface with **OpenCV**

---

## ðŸ§° Requirements

### Python Libraries

Clone the repository:

```bash
git clone https://github.com/HyAgOsK/Drowsy_detection.git
cd Drowsy_detection/
```

Create and activate a virtual environment (recommended for Raspberry Pi 4/5):

```bash
python -m venv venv
source venv/bin/activate
```

Install dependencies:

```bash
pip install -r requirements.txt
```

Active the default performace EdgeTPU Coral USB

```bash
silvatpu-linux-setup
```

Activate the `max` performace of EdgeTPU Coral USB

```bash
silvatpu-linux-setup --speed max

```

> âš ï¸ **Note:** Installing `dlib` requires **CMake** and a compatible C++ compiler.
> 
### Other Prerequisites

- Python 3.8 or higher  
- Coral EdgeTPU (optional, for model acceleration)  
- Webcam or video file  
- Required files:
  - `shape_predictor_68_face_landmarks.dat` (Dlib facial landmark model)
  - `alarm.wav` (alert sound)
  - Trained YOLO model (`best.pt`) .pt .tflite
  - YOLO tracking configuration file: `botsort.yaml` (if using `track()`)

---

## ðŸ—‚ï¸ Expected Folder Structure

```
project/
â”‚
â”œâ”€â”€ alarm.wav
â”œâ”€â”€ shape_predictor_68_face_landmarks.dat
â”œâ”€â”€ main_final.py                    # main script
â”œâ”€â”€ botsort.yaml              # YOLO tracker configuration
â”œâ”€â”€ runs_yolov5/
â”‚   â””â”€â”€ detect/
â”‚       â””â”€â”€ train/
â”‚           â””â”€â”€ weights/
â”‚               â”œâ”€â”€ best.pt
â”‚               â”œâ”€â”€ model_float32.tflite
â”‚               â”œâ”€â”€ model_int8_edgetpu.tflite
â”‚               â””â”€â”€ ...
```

---

## â–¶ï¸ How to Run

Run the main script:

```bash
python main.py
```

A window will open showing the webcam feed and displaying visual and audible alerts in real time.

---

## ðŸ§  Alert Logic

- **Closed eyes** for 10 consecutive frames (Blink Ratio BR > 4.5) **and** YOLO detects `drowsy` âžœ Alert
- **Head leaning forward** (DSC > 2.9) for 10 frames **and** YOLO detects `drowsy` âžœ Alert
- YOLO detects `drowsy`, `phone`, or `smoking` for 10 frames âžœ Alert
- Any **combination of 2 or more active alerts at the same time** âžœ Triggers an audible alert once

---

## ðŸ“ˆ Performance

- **FPS** displayed in real-time in the terminal  
- Uses **GPU (CUDA)** when available  
- Compatible with **EdgeTPU** when a suitable `.tflite` model is loaded in `main.py`

---

## ðŸ“Œ Final Notes

- Ensure your YOLO model is trained for the expected classes: `drowsy`, `phone`, `smoking`, `seatbelt`, `awake`
- By default, the script uses the webcam (`cv2.VideoCapture(0)`) â€” modify this to load a video file if needed from the root directory

---
